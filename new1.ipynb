{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a533ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note on Symlink Warning:\n",
    "\n",
    "# To permanently suppress the Windows symlink warning, add this line at the start of your script\n",
    "\n",
    "# (before any HF imports) or set it in your environment:\n",
    "\n",
    "# os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc74bad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Python Imports ---\n",
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- LangChain/RAG Component Imports ---\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma # 1. Import ChromaDB\n",
    "from langchain_anthropic import ChatAnthropic        # 2. Import Anthropic Chat Model\n",
    "from langchain.chains import RetrievalQA             # 3. Import RAG Chain\n",
    "\n",
    "# Optional: For Hugging Face login if needed for commercial models\n",
    "from huggingface_hub import login\n",
    "# Other imports from original code (kept but not used for RAG core logic)\n",
    "from numpy.linalg import norm\n",
    "from numpy import dot\n",
    "from PIL import Image\n",
    "import requests \n",
    "\n",
    "# Suppress all warnings for cleaner output\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "# --- 1. Authentication & Configuration ---\n",
    "load_dotenv()\n",
    "huggingface_api_key = os.getenv('HUGGINGFACE_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "HF_MODEL_NAME = \"jinaai/jina-embeddings-v2-base-en\"\n",
    "PERSIST_DIR = \"./chroma_db\" # Directory to save the vector store\n",
    "\n",
    "# Check for API Keys\n",
    "if huggingface_api_key:\n",
    "    # Log in to Hugging Face (optional but good practice)\n",
    "    login(token=huggingface_api_key, add_to_git_credential=False)\n",
    "    print(\"Hugging Face login successful.\")\n",
    "else:\n",
    "    print(\"⚠ HUGGINGFACE_API_KEY not found. Proceeding with public access only.\")\n",
    "\n",
    "if not anthropic_api_key:\n",
    "    print(\"❌ ANTHROPIC_API_KEY not found in .env. Cannot initialize Anthropic LLM.\")\n",
    "    # Exit or raise error if the core component is missing\n",
    "    exit() \n",
    "\n",
    "# --- 2. Data Loading and Chunking ---\n",
    "print(\"\\n--- 2. Data Loading and Chunking ---\")\n",
    "try:\n",
    "    loader = PyPDFLoader('attention.pdf')\n",
    "    docs = loader.load()\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading PDF: {e}. Ensure 'attention.pdf' is in the current directory.\")\n",
    "    exit()\n",
    "\n",
    "# Split documents into chunks for RAG\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "print(f\"Loaded {len(docs)} pages and split into {len(documents)} chunks.\")\n",
    "\n",
    "# --- 3. Initialize Embeddings ---\n",
    "print(\"\\n--- 3. Initialize Embeddings ---\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Initialize the HuggingFaceEmbeddings model (Jina Embeddings)\n",
    "text_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=HF_MODEL_NAME,\n",
    "    model_kwargs={\n",
    "        'device': device,\n",
    "        'trust_remote_code': True # Necessary for Jina models\n",
    "    } \n",
    ")\n",
    "print(f\"Embedding model *{HF_MODEL_NAME}* loaded successfully on {device.upper()}.\")\n",
    "\n",
    "# --- 4. Create and Persist Vector Store (ChromaDB) ---\n",
    "print(\"\\n--- 4. Creating Vector Store (ChromaDB) ---\")\n",
    "\n",
    "# Create a Chroma vector store from the document chunks using the HuggingFace embeddings\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=text_embeddings,\n",
    "    persist_directory=PERSIST_DIR # Persist the database to disk\n",
    ")\n",
    "\n",
    "# NOTE: Chroma.from_documents automatically runs the embeddings and stores them.\n",
    "print(f\"ChromaDB created and persisted at '{PERSIST_DIR}' with {vectordb._collection.count()} documents.\")\n",
    "\n",
    "# --- 5. Initialize Anthropic LLM ---\n",
    "print(\"\\n--- 5. Initializing Anthropic LLM ---\")\n",
    "\n",
    "# Initialize the ChatAnthropic model using the API Key from the environment\n",
    "# We use 'claude-3-sonnet-20240229' or a more current model for good performance.\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-3-sonnet-20240229\", \n",
    "    temperature=0.1, # Low temperature for more factual RAG answers\n",
    "    anthropic_api_key=anthropic_api_key # Explicitly pass the key (optional if env var is set)\n",
    ")\n",
    "print(f\"Anthropic LLM *{llm.model}* initialized.\")\n",
    "\n",
    "# --- 6. Set up the RAG Chain (RetrievalQA) ---\n",
    "print(\"\\n--- 6. Setting up RAG Chain ---\")\n",
    "\n",
    "# Convert the vector store into a Retriever\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3}) # Retrieve top 3 relevant chunks\n",
    "\n",
    "# Create the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", # 'stuff' puts all retrieved docs into the prompt\n",
    "    retriever=retriever, \n",
    "    return_source_documents=True # Get the source chunks that answered the query\n",
    ")\n",
    "print(\"RetrievalQA Chain setup complete.\")\n",
    "\n",
    "# --- 7. Answer Queries ---\n",
    "print(\"\\n--- 7. Answering Queries with Anthropic LLM ---\")\n",
    "\n",
    "# Example Query 1\n",
    "query1 = \"What is the key novelty introduced in this paper?\"\n",
    "print(f\"\\n❓ Query 1: {query1}\")\n",
    "result1 = qa_chain.invoke({\"query\": query1})\n",
    "print(f\"✅ Answer: {result1['result']}\")\n",
    "print(f\"Source Document (Page {result1['source_documents'][0].metadata.get('page', 'N/A')}): {result1['source_documents'][0].page_content[:150]}...\")\n",
    "\n",
    "# Example Query 2\n",
    "query2 = \"Describe the multi-head attention mechanism briefly.\"\n",
    "print(f\"\\n❓ Query 2: {query2}\")\n",
    "result2 = qa_chain.invoke({\"query\": query2})\n",
    "print(f\"✅ Answer: {result2['result']}\")\n",
    "print(f\"Source Document (Page {result2['source_documents'][0].metadata.get('page', 'N/A')}): {result2['source_documents'][0].page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09964349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "\n",
    "# --- LangChain RAG Component Imports ---\n",
    "# Use the same components for data processing and vector store\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFacePipeline  # New LLM Wrapper\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# --- Configuration ---\n",
    "PERSIST_DIR = \"./chroma_db\"\n",
    "# Use a strong open-source model for the LLM\n",
    "LLM_MODEL_ID = \"NousResearch/Meta-Llama-3-8B-Instruct\"\n",
    "# The embedding model can remain the same (Jina embeddings or a simple sentence-transformer)\n",
    "EMBEDDING_MODEL_ID = \"jinaai/jina-embeddings-v2-base-en\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# --- 1. Model Initialization (REPLACED ANTHROPIC) ---\n",
    "print(\"\\n--- 1. Initializing Hugging Face LLM ---\")\n",
    "\n",
    "try:\n",
    "    # 1. Configuration for 4-bit Quantization (Essential for 8B models on consumer GPUs)\n",
    "    nf4_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16 # Use bfloat16 for better performance\n",
    "    )\n",
    "    \n",
    "    # 2. Load Tokenizer and Model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_ID)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        LLM_MODEL_ID,\n",
    "        quantization_config=nf4_config,\n",
    "        device_map=\"auto\" # Automatically maps model layers to available hardware\n",
    "    )\n",
    "\n",
    "    # 3. Create the Hugging Face Pipeline\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=512,      # Max tokens for the answer generation\n",
    "        temperature=0.1,\n",
    "        return_full_text=False   # Ensures only the generated text is returned\n",
    "    )\n",
    "    \n",
    "    # 4. Wrap the pipeline in LangChain's HuggingFacePipeline\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    print(f\"Hugging Face LLM *{LLM_MODEL_ID}* initialized with 4-bit quantization on {device.upper()}.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error initializing Hugging Face model: {e}\")\n",
    "    print(\"Falling back to a small, CPU-friendly model (distilgpt2). Performance will be low.\")\n",
    "    from langchain_huggingface import HuggingFacePipeline\n",
    "    llm = HuggingFacePipeline.from_model_id(\n",
    "        model_id=\"distilgpt2\",\n",
    "        task=\"text-generation\",\n",
    "        pipeline_kwargs={\"max_new_tokens\": 100}\n",
    "    )\n",
    "\n",
    "\n",
    "# --- 2. Data Loading, Chunking, and Embeddings (Using the same logic) ---\n",
    "print(\"\\n--- 2. Setting up RAG Components ---\")\n",
    "\n",
    "# (Skipping PDF loading/chunking for brevity, assuming 'documents' and 'text_embeddings' are ready)\n",
    "# --- Start of placeholder code for RAG components ---\n",
    "from langchain_core.documents import Document\n",
    "docs = [Document(page_content=\"The Transformer paper is called Attention Is All You Need and relies solely on attention mechanisms.\", metadata={\"page\": 0})]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "text_embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_ID)\n",
    "vectordb = Chroma.from_documents(documents=documents, embedding=text_embeddings, persist_directory=PERSIST_DIR)\n",
    "# --- End of placeholder code ---\n",
    "\n",
    "# --- 3. Set up the RAG Chain (RetrievalQA) ---\n",
    "print(\"\\n--- 3. Setting up RAG Chain ---\")\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    return_source_documents=True\n",
    ")\n",
    "print(\"RetrievalQA Chain setup complete.\")\n",
    "\n",
    "\n",
    "# --- 4. Answer Queries ---\n",
    "print(\"\\n--- 4. Answering Queries with Hugging Face LLM ---\")\n",
    "\n",
    "query1 = \"What is the key novelty introduced in this paper?\"\n",
    "print(f\"\\n❓ Query 1: {query1}\")\n",
    "result1 = qa_chain.invoke({\"query\": query1})\n",
    "print(f\"✅ Answer: {result1['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a0795f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
